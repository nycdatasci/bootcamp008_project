{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09218+2.92123e-05\ttest-mlogloss:1.09227+4.62887e-05\n",
      "[25]\ttrain-mlogloss:0.961738+0.000446634\ttest-mlogloss:0.964099+0.000946619\n",
      "[50]\ttrain-mlogloss:0.870211+0.000866484\ttest-mlogloss:0.874754+0.00184524\n",
      "[75]\ttrain-mlogloss:0.80376+0.00117337\ttest-mlogloss:0.810425+0.0024304\n",
      "[100]\ttrain-mlogloss:0.754403+0.00101509\ttest-mlogloss:0.763092+0.00316563\n",
      "[125]\ttrain-mlogloss:0.716681+0.00112371\ttest-mlogloss:0.727386+0.00354754\n",
      "[150]\ttrain-mlogloss:0.687609+0.00108322\ttest-mlogloss:0.700275+0.00408667\n",
      "[175]\ttrain-mlogloss:0.664758+0.00125183\ttest-mlogloss:0.679398+0.00431999\n",
      "[200]\ttrain-mlogloss:0.646384+0.00120897\ttest-mlogloss:0.663022+0.00472184\n",
      "[225]\ttrain-mlogloss:0.63134+0.00128274\ttest-mlogloss:0.649987+0.00493861\n",
      "[250]\ttrain-mlogloss:0.618792+0.00123106\ttest-mlogloss:0.639468+0.00519248\n",
      "[275]\ttrain-mlogloss:0.607965+0.0012318\ttest-mlogloss:0.630693+0.0053663\n",
      "[300]\ttrain-mlogloss:0.598569+0.00113854\ttest-mlogloss:0.623352+0.00561394\n",
      "[325]\ttrain-mlogloss:0.590283+0.00113491\ttest-mlogloss:0.617131+0.00575039\n",
      "[350]\ttrain-mlogloss:0.582912+0.00115963\ttest-mlogloss:0.611852+0.00582388\n",
      "[375]\ttrain-mlogloss:0.576157+0.00108531\ttest-mlogloss:0.60712+0.00599306\n",
      "[400]\ttrain-mlogloss:0.569936+0.00110949\ttest-mlogloss:0.602958+0.00600956\n",
      "[425]\ttrain-mlogloss:0.564216+0.00106572\ttest-mlogloss:0.599312+0.00608696\n",
      "[450]\ttrain-mlogloss:0.558877+0.00102988\ttest-mlogloss:0.595973+0.00614981\n",
      "[475]\ttrain-mlogloss:0.553775+0.000963714\ttest-mlogloss:0.592924+0.00625506\n",
      "[500]\ttrain-mlogloss:0.548992+0.000970704\ttest-mlogloss:0.590154+0.00621555\n",
      "[525]\ttrain-mlogloss:0.544397+0.00101317\ttest-mlogloss:0.58763+0.00620158\n",
      "[550]\ttrain-mlogloss:0.540052+0.00099814\ttest-mlogloss:0.585286+0.00617486\n",
      "[575]\ttrain-mlogloss:0.535895+0.00108744\ttest-mlogloss:0.583137+0.00614658\n",
      "[600]\ttrain-mlogloss:0.531964+0.00108301\ttest-mlogloss:0.581127+0.00620618\n",
      "[625]\ttrain-mlogloss:0.528179+0.00111304\ttest-mlogloss:0.579269+0.0061725\n",
      "[650]\ttrain-mlogloss:0.524659+0.00112336\ttest-mlogloss:0.577601+0.00611293\n",
      "[675]\ttrain-mlogloss:0.521257+0.00108884\ttest-mlogloss:0.576044+0.00610691\n",
      "[700]\ttrain-mlogloss:0.517888+0.00113629\ttest-mlogloss:0.574517+0.00606192\n",
      "[725]\ttrain-mlogloss:0.514632+0.00107428\ttest-mlogloss:0.573104+0.00609409\n",
      "[750]\ttrain-mlogloss:0.511466+0.00104028\ttest-mlogloss:0.571762+0.00613832\n",
      "[775]\ttrain-mlogloss:0.50849+0.000999936\ttest-mlogloss:0.570548+0.00617815\n",
      "[800]\ttrain-mlogloss:0.505584+0.00102652\ttest-mlogloss:0.569417+0.00618437\n",
      "[825]\ttrain-mlogloss:0.502852+0.000972416\ttest-mlogloss:0.56834+0.0062569\n",
      "[850]\ttrain-mlogloss:0.500074+0.000983424\ttest-mlogloss:0.567321+0.0062749\n",
      "[875]\ttrain-mlogloss:0.497384+0.000934518\ttest-mlogloss:0.566324+0.00630603\n",
      "[900]\ttrain-mlogloss:0.494877+0.000982703\ttest-mlogloss:0.565432+0.0063246\n",
      "[925]\ttrain-mlogloss:0.492339+0.000994553\ttest-mlogloss:0.564561+0.00631219\n",
      "[950]\ttrain-mlogloss:0.489818+0.000954005\ttest-mlogloss:0.563713+0.00636517\n",
      "[975]\ttrain-mlogloss:0.487468+0.000907966\ttest-mlogloss:0.562934+0.00638799\n",
      "[1000]\ttrain-mlogloss:0.485074+0.000948267\ttest-mlogloss:0.562146+0.00640146\n",
      "[1025]\ttrain-mlogloss:0.482721+0.000962548\ttest-mlogloss:0.561404+0.00639724\n",
      "[1050]\ttrain-mlogloss:0.480429+0.000996336\ttest-mlogloss:0.560688+0.00643772\n",
      "[1075]\ttrain-mlogloss:0.478221+0.000991911\ttest-mlogloss:0.560061+0.00644295\n",
      "[1100]\ttrain-mlogloss:0.47604+0.000939876\ttest-mlogloss:0.559403+0.00650075\n",
      "[1125]\ttrain-mlogloss:0.473915+0.000888435\ttest-mlogloss:0.558807+0.00655846\n",
      "[1150]\ttrain-mlogloss:0.471796+0.000896948\ttest-mlogloss:0.558236+0.00657319\n",
      "[1175]\ttrain-mlogloss:0.46972+0.000910235\ttest-mlogloss:0.557683+0.00657142\n",
      "[1200]\ttrain-mlogloss:0.467681+0.000943562\ttest-mlogloss:0.557123+0.00653669\n",
      "[1225]\ttrain-mlogloss:0.465664+0.000962822\ttest-mlogloss:0.556614+0.00652007\n",
      "[1250]\ttrain-mlogloss:0.46365+0.000970278\ttest-mlogloss:0.556112+0.00652785\n",
      "[1275]\ttrain-mlogloss:0.461694+0.000965628\ttest-mlogloss:0.555651+0.00655347\n",
      "[1300]\ttrain-mlogloss:0.459747+0.000953785\ttest-mlogloss:0.555177+0.00658458\n",
      "[1325]\ttrain-mlogloss:0.457852+0.0010046\ttest-mlogloss:0.554767+0.00660965\n",
      "[1350]\ttrain-mlogloss:0.455938+0.00103923\ttest-mlogloss:0.554328+0.00663644\n",
      "[1375]\ttrain-mlogloss:0.454102+0.000995246\ttest-mlogloss:0.553945+0.00666828\n",
      "[1400]\ttrain-mlogloss:0.452286+0.0010383\ttest-mlogloss:0.553521+0.00670235\n",
      "[1425]\ttrain-mlogloss:0.450539+0.00106449\ttest-mlogloss:0.553165+0.00670226\n",
      "[1450]\ttrain-mlogloss:0.448709+0.00105887\ttest-mlogloss:0.552773+0.00674228\n",
      "[1475]\ttrain-mlogloss:0.446967+0.00105864\ttest-mlogloss:0.55241+0.00678751\n",
      "[1500]\ttrain-mlogloss:0.445248+0.00105936\ttest-mlogloss:0.552054+0.00678894\n",
      "[1525]\ttrain-mlogloss:0.443561+0.00106066\ttest-mlogloss:0.551726+0.0067922\n",
      "[1550]\ttrain-mlogloss:0.441863+0.00101825\ttest-mlogloss:0.551391+0.00681365\n",
      "[1575]\ttrain-mlogloss:0.440184+0.00104066\ttest-mlogloss:0.551082+0.00682785\n",
      "[1600]\ttrain-mlogloss:0.438505+0.00105696\ttest-mlogloss:0.550791+0.0068498\n",
      "[1625]\ttrain-mlogloss:0.436898+0.00105455\ttest-mlogloss:0.550481+0.00688452\n",
      "[1650]\ttrain-mlogloss:0.435269+0.00107937\ttest-mlogloss:0.550159+0.00690412\n",
      "[1675]\ttrain-mlogloss:0.433633+0.00108649\ttest-mlogloss:0.549877+0.00690593\n",
      "[1700]\ttrain-mlogloss:0.432016+0.00113076\ttest-mlogloss:0.549594+0.00691212\n",
      "[1725]\ttrain-mlogloss:0.430457+0.00114373\ttest-mlogloss:0.549374+0.00690713\n",
      "[1750]\ttrain-mlogloss:0.428894+0.0011548\ttest-mlogloss:0.549129+0.00694384\n",
      "[1775]\ttrain-mlogloss:0.427326+0.001165\ttest-mlogloss:0.548872+0.00694997\n",
      "[1800]\ttrain-mlogloss:0.42579+0.00115628\ttest-mlogloss:0.548651+0.00695013\n",
      "[1825]\ttrain-mlogloss:0.424268+0.00111317\ttest-mlogloss:0.548407+0.00696908\n",
      "[1850]\ttrain-mlogloss:0.422702+0.00114318\ttest-mlogloss:0.548159+0.00697514\n",
      "[1875]\ttrain-mlogloss:0.421214+0.00116706\ttest-mlogloss:0.547955+0.00699103\n",
      "[1900]\ttrain-mlogloss:0.419726+0.00116258\ttest-mlogloss:0.547754+0.00700237\n",
      "[1925]\ttrain-mlogloss:0.418235+0.00113512\ttest-mlogloss:0.547535+0.00702351\n",
      "[1950]\ttrain-mlogloss:0.416753+0.00112859\ttest-mlogloss:0.547329+0.00701387\n",
      "[1975]\ttrain-mlogloss:0.415308+0.0011836\ttest-mlogloss:0.54715+0.00700618\n",
      "[2000]\ttrain-mlogloss:0.413872+0.00118758\ttest-mlogloss:0.546999+0.00702253\n",
      "[2025]\ttrain-mlogloss:0.412405+0.00123437\ttest-mlogloss:0.546849+0.00703591\n",
      "[2050]\ttrain-mlogloss:0.410954+0.00123387\ttest-mlogloss:0.546672+0.00705806\n",
      "[2075]\ttrain-mlogloss:0.409551+0.0012483\ttest-mlogloss:0.546513+0.00708842\n",
      "[2100]\ttrain-mlogloss:0.408131+0.00123429\ttest-mlogloss:0.546331+0.00713158\n",
      "[2125]\ttrain-mlogloss:0.406726+0.00122859\ttest-mlogloss:0.546144+0.00715651\n",
      "[2150]\ttrain-mlogloss:0.40536+0.00124768\ttest-mlogloss:0.545985+0.00715078\n",
      "[2175]\ttrain-mlogloss:0.403957+0.00126811\ttest-mlogloss:0.545838+0.00720339\n",
      "[2200]\ttrain-mlogloss:0.402571+0.0012589\ttest-mlogloss:0.545679+0.00722999\n",
      "[2225]\ttrain-mlogloss:0.401203+0.00124954\ttest-mlogloss:0.545535+0.00723508\n",
      "[2250]\ttrain-mlogloss:0.399883+0.00125328\ttest-mlogloss:0.545423+0.0072369\n",
      "[2275]\ttrain-mlogloss:0.398537+0.00127314\ttest-mlogloss:0.545282+0.00724275\n",
      "[2300]\ttrain-mlogloss:0.397218+0.00126275\ttest-mlogloss:0.545151+0.00724922\n",
      "[2325]\ttrain-mlogloss:0.395878+0.00127992\ttest-mlogloss:0.545007+0.00724685\n",
      "[2350]\ttrain-mlogloss:0.394569+0.00132556\ttest-mlogloss:0.544863+0.00725621\n",
      "[2375]\ttrain-mlogloss:0.393272+0.00138886\ttest-mlogloss:0.544755+0.00725759\n",
      "[2400]\ttrain-mlogloss:0.391982+0.00143352\ttest-mlogloss:0.544634+0.00725629\n",
      "[2425]\ttrain-mlogloss:0.390704+0.00145865\ttest-mlogloss:0.544514+0.00728177\n",
      "[2450]\ttrain-mlogloss:0.389418+0.0014478\ttest-mlogloss:0.544409+0.00728156\n",
      "[2475]\ttrain-mlogloss:0.38815+0.00141282\ttest-mlogloss:0.544291+0.00727854\n",
      "[2500]\ttrain-mlogloss:0.386904+0.00140889\ttest-mlogloss:0.544179+0.00729862\n",
      "[2525]\ttrain-mlogloss:0.385624+0.00147273\ttest-mlogloss:0.544094+0.007284\n",
      "[2550]\ttrain-mlogloss:0.384372+0.0015137\ttest-mlogloss:0.543979+0.00728359\n",
      "[2575]\ttrain-mlogloss:0.383109+0.00152844\ttest-mlogloss:0.543888+0.00729196\n",
      "[2600]\ttrain-mlogloss:0.38186+0.00158574\ttest-mlogloss:0.543794+0.00730109\n",
      "[2625]\ttrain-mlogloss:0.380605+0.00158646\ttest-mlogloss:0.543702+0.00734037\n",
      "[2650]\ttrain-mlogloss:0.379368+0.00158822\ttest-mlogloss:0.54362+0.00736971\n",
      "[2675]\ttrain-mlogloss:0.378164+0.00160517\ttest-mlogloss:0.543531+0.00738169\n",
      "[2700]\ttrain-mlogloss:0.376968+0.00161787\ttest-mlogloss:0.543456+0.00739485\n",
      "[2725]\ttrain-mlogloss:0.375772+0.00162564\ttest-mlogloss:0.543386+0.00740193\n",
      "[2750]\ttrain-mlogloss:0.374555+0.00163808\ttest-mlogloss:0.543336+0.00741609\n",
      "[2775]\ttrain-mlogloss:0.373344+0.00165861\ttest-mlogloss:0.543251+0.00744745\n",
      "[2800]\ttrain-mlogloss:0.372124+0.00168341\ttest-mlogloss:0.543174+0.00742593\n",
      "[2825]\ttrain-mlogloss:0.370966+0.00170217\ttest-mlogloss:0.543087+0.00743509\n",
      "[2850]\ttrain-mlogloss:0.369789+0.00173595\ttest-mlogloss:0.543003+0.00745247\n",
      "[2875]\ttrain-mlogloss:0.368672+0.00171003\ttest-mlogloss:0.54295+0.00745135\n",
      "[2900]\ttrain-mlogloss:0.367436+0.00174456\ttest-mlogloss:0.542878+0.00742915\n",
      "[2925]\ttrain-mlogloss:0.36625+0.00173846\ttest-mlogloss:0.542825+0.00743864\n",
      "[2950]\ttrain-mlogloss:0.36508+0.00176444\ttest-mlogloss:0.542776+0.00745395\n",
      "[2975]\ttrain-mlogloss:0.363931+0.00180183\ttest-mlogloss:0.542727+0.0074699\n",
      "[3000]\ttrain-mlogloss:0.362757+0.00177233\ttest-mlogloss:0.542663+0.00747487\n",
      "[3025]\ttrain-mlogloss:0.361576+0.00176882\ttest-mlogloss:0.542589+0.00749685\n",
      "[3050]\ttrain-mlogloss:0.360441+0.00175653\ttest-mlogloss:0.542538+0.0075307\n",
      "[3075]\ttrain-mlogloss:0.359299+0.00178345\ttest-mlogloss:0.542483+0.00753735\n",
      "[3100]\ttrain-mlogloss:0.358133+0.00176187\ttest-mlogloss:0.542449+0.00755233\n",
      "[3125]\ttrain-mlogloss:0.357027+0.00176454\ttest-mlogloss:0.542408+0.00756888\n",
      "[3150]\ttrain-mlogloss:0.355887+0.00173173\ttest-mlogloss:0.542371+0.00759376\n",
      "[3175]\ttrain-mlogloss:0.354768+0.00174581\ttest-mlogloss:0.542349+0.00759511\n",
      "[3200]\ttrain-mlogloss:0.353639+0.00179388\ttest-mlogloss:0.542319+0.00757687\n",
      "[3225]\ttrain-mlogloss:0.352518+0.00179825\ttest-mlogloss:0.542282+0.00759317\n",
      "[3250]\ttrain-mlogloss:0.351404+0.00180231\ttest-mlogloss:0.542238+0.00760463\n",
      "[3275]\ttrain-mlogloss:0.350311+0.00182958\ttest-mlogloss:0.542204+0.00762519\n",
      "[3300]\ttrain-mlogloss:0.349247+0.00182799\ttest-mlogloss:0.542194+0.00761804\n",
      "[3325]\ttrain-mlogloss:0.348131+0.00184719\ttest-mlogloss:0.54218+0.00761628\n",
      "[3350]\ttrain-mlogloss:0.347034+0.00185967\ttest-mlogloss:0.542144+0.00762552\n",
      "[3375]\ttrain-mlogloss:0.345929+0.00189521\ttest-mlogloss:0.542136+0.00764903\n",
      "[3400]\ttrain-mlogloss:0.344831+0.0019097\ttest-mlogloss:0.542086+0.00768033\n",
      "[3425]\ttrain-mlogloss:0.343751+0.00189308\ttest-mlogloss:0.542045+0.00768431\n",
      "[3450]\ttrain-mlogloss:0.342684+0.00190166\ttest-mlogloss:0.542031+0.00771081\n",
      "[3475]\ttrain-mlogloss:0.341606+0.00192403\ttest-mlogloss:0.541993+0.00771474\n",
      "[3500]\ttrain-mlogloss:0.340563+0.00192246\ttest-mlogloss:0.541978+0.0077077\n",
      "[3525]\ttrain-mlogloss:0.339516+0.00192836\ttest-mlogloss:0.541978+0.00773119\n",
      "[3550]\ttrain-mlogloss:0.338456+0.00190446\ttest-mlogloss:0.541956+0.00774243\n",
      "[3575]\ttrain-mlogloss:0.337422+0.00187448\ttest-mlogloss:0.54195+0.00775168\n",
      "[3600]\ttrain-mlogloss:0.336377+0.00187786\ttest-mlogloss:0.541937+0.00777282\n",
      "[3625]\ttrain-mlogloss:0.335339+0.00189342\ttest-mlogloss:0.541931+0.00780261\n",
      "[3650]\ttrain-mlogloss:0.334305+0.00192528\ttest-mlogloss:0.541918+0.007814\n",
      "[3675]\ttrain-mlogloss:0.333288+0.00190393\ttest-mlogloss:0.5419+0.00783116\n",
      "[3700]\ttrain-mlogloss:0.332275+0.00192548\ttest-mlogloss:0.54191+0.00781837\n",
      "[3725]\ttrain-mlogloss:0.331255+0.00188237\ttest-mlogloss:0.5419+0.00784897\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'Dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1fb3af38f266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'high'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'medium'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'low'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'Dataframe'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing, model_selection\n",
    "import string\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from scipy.stats import boxcox\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_json('train.json')\n",
    "test = pd.read_json('test.json')\n",
    "listing_id = test.listing_id.values\n",
    "\n",
    "\n",
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "train['interest_level'] = train['interest_level'].apply(lambda x: y_map[x])\n",
    "y_train = train.interest_level.values\n",
    "\n",
    "train = train.drop(['listing_id', 'interest_level'], axis=1)\n",
    "test = test.drop('listing_id', axis=1)\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "\n",
    "train_test = pd.concat((train, test), axis=0).reset_index(drop=True)\n",
    "\n",
    "train_test['Date'] = pd.to_datetime(train_test['created'])\n",
    "train_test['Year'] = train_test['Date'].dt.year\n",
    "train_test['Month'] = train_test['Date'].dt.month\n",
    "train_test['Day'] = train_test['Date'].dt.day\n",
    "train_test['Wday'] = train_test['Date'].dt.dayofweek\n",
    "train_test['Yday'] = train_test['Date'].dt.dayofyear\n",
    "train_test['hour'] = train_test['Date'].dt.hour\n",
    "\n",
    "train_test = train_test.drop(['Date', 'created'], axis=1)\n",
    "\n",
    "train_test['Zero_building_id'] = train_test['building_id'].apply(lambda x: 1 if x == '0' else 0)\n",
    "\n",
    "train_test['desc'] = train_test['description']\n",
    "train_test['desc'] = train_test['desc'].apply(lambda x: x.replace('<p><a  website_redacted ', ''))\n",
    "train_test['desc'] = train_test['desc'].apply(lambda x: x.replace('!<br /><br />', ''))\n",
    "\n",
    "string.punctuation.__add__('!!')\n",
    "string.punctuation.__add__('(')\n",
    "string.punctuation.__add__(')')\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "train_test['desc'] = train_test['desc'].apply(lambda x: x.translate(remove_punct_map))\n",
    "train_test['desc_letters_count'] = train_test['description'].apply(lambda x: len(x.strip()))\n",
    "train_test['desc_words_count'] = train_test['desc'].apply(lambda x: 0 if len(x.strip()) == 0 else len(x.split(' ')))\n",
    "\n",
    "train_test.drop(['description', 'desc'], axis=1, inplace=True)\n",
    "\n",
    "train_test['address1'] = train_test['display_address']\n",
    "train_test['address1'] = train_test['address1'].apply(lambda x: x.lower())\n",
    "\n",
    "address_map = {\n",
    "    'w': 'west',\n",
    "    'st.': 'street',\n",
    "    'ave': 'avenue',\n",
    "    'st': 'street',\n",
    "    'e': 'east',\n",
    "    'n': 'north',\n",
    "    's': 'south'\n",
    "}\n",
    "\n",
    "\n",
    "def address_map_func(s):\n",
    "    s = s.split(' ')\n",
    "    out = []\n",
    "    for x in s:\n",
    "        if x in address_map:\n",
    "            out.append(address_map[x])\n",
    "        else:\n",
    "            out.append(x)\n",
    "    return ' '.join(out)\n",
    "\n",
    "\n",
    "train_test['address1'] = train_test['address1'].apply(lambda x: x.translate(remove_punct_map))\n",
    "train_test['address1'] = train_test['address1'].apply(lambda x: address_map_func(x))\n",
    "\n",
    "new_cols = ['street', 'avenue', 'east', 'west', 'north', 'south']\n",
    "\n",
    "for col in new_cols:\n",
    "    train_test[col] = train_test['address1'].apply(lambda x: 1 if col in x else 0)\n",
    "\n",
    "train_test['other_address'] = train_test[new_cols].apply(lambda x: 1 if x.sum() == 0 else 0, axis=1)\n",
    "\n",
    "train_test['features_count'] = train_test['features'].apply(lambda x: len(x))\n",
    "train_test['features2'] = train_test['features']\n",
    "train_test['features2'] = train_test['features2'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "c_vect = CountVectorizer(stop_words='english', max_features=200, ngram_range=(1, 1))\n",
    "c_vect.fit(train_test['features2'])\n",
    "\n",
    "c_vect_sparse_1 = c_vect.transform(train_test['features2'])\n",
    "c_vect_sparse1_cols = c_vect.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "train_test.drop(['features', 'features2'], axis=1, inplace=True)\n",
    "\n",
    "managers_count = train_test['manager_id'].value_counts()\n",
    "\n",
    "train_test['top_10_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 90)] else 0)\n",
    "train_test['top_25_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 75)] else 0)\n",
    "train_test['top_5_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 95)] else 0)\n",
    "train_test['top_50_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 50)] else 0)\n",
    "train_test['top_1_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 99)] else 0)\n",
    "train_test['top_2_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 98)] else 0)\n",
    "train_test['top_15_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 85)] else 0)\n",
    "train_test['top_20_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 80)] else 0)\n",
    "train_test['top_30_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 70)] else 0)\n",
    "\n",
    "buildings_count = train_test['building_id'].value_counts()\n",
    "\n",
    "train_test['top_10_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 90)] else 0)\n",
    "train_test['top_25_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 75)] else 0)\n",
    "train_test['top_5_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 95)] else 0)\n",
    "train_test['top_50_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 50)] else 0)\n",
    "train_test['top_1_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 99)] else 0)\n",
    "train_test['top_2_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 98)] else 0)\n",
    "train_test['top_15_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 85)] else 0)\n",
    "train_test['top_20_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 80)] else 0)\n",
    "train_test['top_30_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 70)] else 0)\n",
    "\n",
    "train_test['photos_count'] = train_test['photos'].apply(lambda x: len(x))\n",
    "train_test.drop(['photos', 'display_address', 'street_address'], axis=1, inplace=True)\n",
    "\n",
    "categoricals = [x for x in train_test.columns if train_test[x].dtype == 'object']\n",
    "\n",
    "for feat in categoricals:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train_test[feat].values))\n",
    "    train_test[feat] = lbl.transform(list(train_test[feat].values))\n",
    "\n",
    "bc_price, tmp = boxcox(train_test.price)\n",
    "train_test['bc_price'] = bc_price\n",
    "\n",
    "train_test.drop('price', axis=1, inplace=True)\n",
    "train_test['bathrooms_cat'] = train_test['bathrooms'].apply(lambda x: str(x))\n",
    "\n",
    "train_test['bathrooms_cat'], labels = pd.factorize(train_test['bathrooms_cat'].values, sort=True)\n",
    "train_test.drop('bathrooms', axis=1, inplace=True)\n",
    "\n",
    "train_test['bedroom_cat'], labels = pd.factorize(train_test['bedrooms'].values, sort=True)\n",
    "train_test.drop('bedrooms', axis=1, inplace=True)\n",
    "\n",
    "features = list(train_test.columns)\n",
    "\n",
    "\n",
    "train_test_cv1_sparse = sparse.hstack((train_test, c_vect_sparse_1)).tocsr()\n",
    "\n",
    "\n",
    "x_train = train_test_cv1_sparse[:ntrain, :]\n",
    "x_test = train_test_cv1_sparse[ntrain:, :]\n",
    "features += c_vect_sparse1_cols\n",
    "\n",
    "SEED = 777\n",
    "NFOLDS = 5\n",
    "\n",
    "params = {\n",
    "    'eta':.01,\n",
    "    'colsample_bytree':.8,\n",
    "    'subsample':.8,\n",
    "    'seed':0,\n",
    "    'nthread':16,\n",
    "    'objective':'multi:softprob',\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class':3,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=x_test)\n",
    "\n",
    "\n",
    "bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=25)\n",
    "\n",
    "best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "\n",
    "bst = xgb.train(params, dtrain, best_rounds)\n",
    "\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "preds = pd.DataFrame(preds)\n",
    "\n",
    "cols = ['high', 'medium', 'low']\n",
    "\n",
    "preds.columns = cols\n",
    "\n",
    "preds['listing_id'] = listing_id\n",
    "\n",
    "preds.to_csv('my_preds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>medium</th>\n",
       "      <th>low</th>\n",
       "      <th>listing_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.077437</td>\n",
       "      <td>0.477482</td>\n",
       "      <td>0.445081</td>\n",
       "      <td>7142618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.071715</td>\n",
       "      <td>0.129616</td>\n",
       "      <td>0.798669</td>\n",
       "      <td>7210040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018787</td>\n",
       "      <td>0.297375</td>\n",
       "      <td>0.683838</td>\n",
       "      <td>7103890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038885</td>\n",
       "      <td>0.404470</td>\n",
       "      <td>0.556645</td>\n",
       "      <td>7143442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017555</td>\n",
       "      <td>0.177451</td>\n",
       "      <td>0.804994</td>\n",
       "      <td>6860601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.020269</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>6840081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.020707</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.737334</td>\n",
       "      <td>6922337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.046263</td>\n",
       "      <td>0.371837</td>\n",
       "      <td>0.581899</td>\n",
       "      <td>6913616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.043775</td>\n",
       "      <td>0.425206</td>\n",
       "      <td>0.531019</td>\n",
       "      <td>6937820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.026521</td>\n",
       "      <td>0.352560</td>\n",
       "      <td>0.620919</td>\n",
       "      <td>6893933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.003839</td>\n",
       "      <td>0.028644</td>\n",
       "      <td>0.967517</td>\n",
       "      <td>6832604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.058812</td>\n",
       "      <td>0.326265</td>\n",
       "      <td>0.614923</td>\n",
       "      <td>6915282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.070895</td>\n",
       "      <td>0.388016</td>\n",
       "      <td>0.541090</td>\n",
       "      <td>7127565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.017052</td>\n",
       "      <td>0.980937</td>\n",
       "      <td>6827899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.008580</td>\n",
       "      <td>0.991196</td>\n",
       "      <td>6934855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.006948</td>\n",
       "      <td>0.176044</td>\n",
       "      <td>0.817008</td>\n",
       "      <td>6861826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.074136</td>\n",
       "      <td>0.430336</td>\n",
       "      <td>0.495528</td>\n",
       "      <td>6871643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.010290</td>\n",
       "      <td>0.989473</td>\n",
       "      <td>6842542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.045727</td>\n",
       "      <td>0.952272</td>\n",
       "      <td>6934145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.058883</td>\n",
       "      <td>0.392765</td>\n",
       "      <td>0.548352</td>\n",
       "      <td>6829365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.025314</td>\n",
       "      <td>0.266762</td>\n",
       "      <td>0.707924</td>\n",
       "      <td>7167858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.090350</td>\n",
       "      <td>0.906423</td>\n",
       "      <td>6859483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.022181</td>\n",
       "      <td>0.113275</td>\n",
       "      <td>0.864545</td>\n",
       "      <td>6861377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.042207</td>\n",
       "      <td>0.243934</td>\n",
       "      <td>0.713860</td>\n",
       "      <td>6848960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.016005</td>\n",
       "      <td>0.983517</td>\n",
       "      <td>6918850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.201948</td>\n",
       "      <td>0.654414</td>\n",
       "      <td>0.143638</td>\n",
       "      <td>6916867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.013860</td>\n",
       "      <td>0.147319</td>\n",
       "      <td>0.838821</td>\n",
       "      <td>6895840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.460839</td>\n",
       "      <td>0.326937</td>\n",
       "      <td>0.212224</td>\n",
       "      <td>6813539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.105256</td>\n",
       "      <td>0.328556</td>\n",
       "      <td>0.566187</td>\n",
       "      <td>7116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.003526</td>\n",
       "      <td>0.007835</td>\n",
       "      <td>0.988639</td>\n",
       "      <td>6890328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74629</th>\n",
       "      <td>0.062848</td>\n",
       "      <td>0.449396</td>\n",
       "      <td>0.487756</td>\n",
       "      <td>6855560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74630</th>\n",
       "      <td>0.065613</td>\n",
       "      <td>0.489603</td>\n",
       "      <td>0.444784</td>\n",
       "      <td>6816731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74631</th>\n",
       "      <td>0.011603</td>\n",
       "      <td>0.060115</td>\n",
       "      <td>0.928283</td>\n",
       "      <td>6925764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74632</th>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.033666</td>\n",
       "      <td>0.963210</td>\n",
       "      <td>7139280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74633</th>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.217322</td>\n",
       "      <td>0.780503</td>\n",
       "      <td>6913068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74634</th>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.032403</td>\n",
       "      <td>0.966672</td>\n",
       "      <td>6828445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74635</th>\n",
       "      <td>0.140230</td>\n",
       "      <td>0.265505</td>\n",
       "      <td>0.594266</td>\n",
       "      <td>6867865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74636</th>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.021286</td>\n",
       "      <td>0.977817</td>\n",
       "      <td>6820397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74637</th>\n",
       "      <td>0.037106</td>\n",
       "      <td>0.214124</td>\n",
       "      <td>0.748769</td>\n",
       "      <td>6852197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74638</th>\n",
       "      <td>0.114443</td>\n",
       "      <td>0.317302</td>\n",
       "      <td>0.568254</td>\n",
       "      <td>7122934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74639</th>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>0.996755</td>\n",
       "      <td>6907838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74640</th>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>0.994817</td>\n",
       "      <td>6865896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74641</th>\n",
       "      <td>0.117541</td>\n",
       "      <td>0.371559</td>\n",
       "      <td>0.510900</td>\n",
       "      <td>6840250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74642</th>\n",
       "      <td>0.023852</td>\n",
       "      <td>0.196786</td>\n",
       "      <td>0.779362</td>\n",
       "      <td>6926011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74643</th>\n",
       "      <td>0.009745</td>\n",
       "      <td>0.124940</td>\n",
       "      <td>0.865315</td>\n",
       "      <td>6893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74644</th>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.024571</td>\n",
       "      <td>0.975138</td>\n",
       "      <td>6867538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74645</th>\n",
       "      <td>0.380060</td>\n",
       "      <td>0.462428</td>\n",
       "      <td>0.157512</td>\n",
       "      <td>6884360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74646</th>\n",
       "      <td>0.051656</td>\n",
       "      <td>0.533416</td>\n",
       "      <td>0.414928</td>\n",
       "      <td>6903964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74647</th>\n",
       "      <td>0.042636</td>\n",
       "      <td>0.214531</td>\n",
       "      <td>0.742833</td>\n",
       "      <td>6907851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74648</th>\n",
       "      <td>0.057128</td>\n",
       "      <td>0.422862</td>\n",
       "      <td>0.520010</td>\n",
       "      <td>7211166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74649</th>\n",
       "      <td>0.400608</td>\n",
       "      <td>0.474318</td>\n",
       "      <td>0.125074</td>\n",
       "      <td>6844290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74650</th>\n",
       "      <td>0.282737</td>\n",
       "      <td>0.540219</td>\n",
       "      <td>0.177044</td>\n",
       "      <td>6947597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74651</th>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.160928</td>\n",
       "      <td>0.838712</td>\n",
       "      <td>6895423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74652</th>\n",
       "      <td>0.007261</td>\n",
       "      <td>0.037420</td>\n",
       "      <td>0.955319</td>\n",
       "      <td>6812077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74653</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.997996</td>\n",
       "      <td>6903956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74654</th>\n",
       "      <td>0.024991</td>\n",
       "      <td>0.076236</td>\n",
       "      <td>0.898773</td>\n",
       "      <td>6881005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74655</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.999477</td>\n",
       "      <td>6835379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74656</th>\n",
       "      <td>0.054885</td>\n",
       "      <td>0.281197</td>\n",
       "      <td>0.663918</td>\n",
       "      <td>6882352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74657</th>\n",
       "      <td>0.480274</td>\n",
       "      <td>0.446461</td>\n",
       "      <td>0.073265</td>\n",
       "      <td>6884758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74658</th>\n",
       "      <td>0.011743</td>\n",
       "      <td>0.110521</td>\n",
       "      <td>0.877736</td>\n",
       "      <td>6924212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74659 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           high    medium       low  listing_id\n",
       "0      0.077437  0.477482  0.445081     7142618\n",
       "1      0.071715  0.129616  0.798669     7210040\n",
       "2      0.018787  0.297375  0.683838     7103890\n",
       "3      0.038885  0.404470  0.556645     7143442\n",
       "4      0.017555  0.177451  0.804994     6860601\n",
       "5      0.000468  0.020269  0.979263     6840081\n",
       "6      0.020707  0.241958  0.737334     6922337\n",
       "7      0.046263  0.371837  0.581899     6913616\n",
       "8      0.043775  0.425206  0.531019     6937820\n",
       "9      0.026521  0.352560  0.620919     6893933\n",
       "10     0.003839  0.028644  0.967517     6832604\n",
       "11     0.058812  0.326265  0.614923     6915282\n",
       "12     0.070895  0.388016  0.541090     7127565\n",
       "13     0.002012  0.017052  0.980937     6827899\n",
       "14     0.000224  0.008580  0.991196     6934855\n",
       "15     0.006948  0.176044  0.817008     6861826\n",
       "16     0.074136  0.430336  0.495528     6871643\n",
       "17     0.000237  0.010290  0.989473     6842542\n",
       "18     0.002001  0.045727  0.952272     6934145\n",
       "19     0.058883  0.392765  0.548352     6829365\n",
       "20     0.025314  0.266762  0.707924     7167858\n",
       "21     0.003228  0.090350  0.906423     6859483\n",
       "22     0.022181  0.113275  0.864545     6861377\n",
       "23     0.042207  0.243934  0.713860     6848960\n",
       "24     0.000478  0.016005  0.983517     6918850\n",
       "25     0.201948  0.654414  0.143638     6916867\n",
       "26     0.013860  0.147319  0.838821     6895840\n",
       "27     0.460839  0.326937  0.212224     6813539\n",
       "28     0.105256  0.328556  0.566187     7116900\n",
       "29     0.003526  0.007835  0.988639     6890328\n",
       "...         ...       ...       ...         ...\n",
       "74629  0.062848  0.449396  0.487756     6855560\n",
       "74630  0.065613  0.489603  0.444784     6816731\n",
       "74631  0.011603  0.060115  0.928283     6925764\n",
       "74632  0.003125  0.033666  0.963210     7139280\n",
       "74633  0.002175  0.217322  0.780503     6913068\n",
       "74634  0.000925  0.032403  0.966672     6828445\n",
       "74635  0.140230  0.265505  0.594266     6867865\n",
       "74636  0.000897  0.021286  0.977817     6820397\n",
       "74637  0.037106  0.214124  0.748769     6852197\n",
       "74638  0.114443  0.317302  0.568254     7122934\n",
       "74639  0.000147  0.003098  0.996755     6907838\n",
       "74640  0.000142  0.005041  0.994817     6865896\n",
       "74641  0.117541  0.371559  0.510900     6840250\n",
       "74642  0.023852  0.196786  0.779362     6926011\n",
       "74643  0.009745  0.124940  0.865315     6893100\n",
       "74644  0.000291  0.024571  0.975138     6867538\n",
       "74645  0.380060  0.462428  0.157512     6884360\n",
       "74646  0.051656  0.533416  0.414928     6903964\n",
       "74647  0.042636  0.214531  0.742833     6907851\n",
       "74648  0.057128  0.422862  0.520010     7211166\n",
       "74649  0.400608  0.474318  0.125074     6844290\n",
       "74650  0.282737  0.540219  0.177044     6947597\n",
       "74651  0.000360  0.160928  0.838712     6895423\n",
       "74652  0.007261  0.037420  0.955319     6812077\n",
       "74653  0.000043  0.001961  0.997996     6903956\n",
       "74654  0.024991  0.076236  0.898773     6881005\n",
       "74655  0.000017  0.000506  0.999477     6835379\n",
       "74656  0.054885  0.281197  0.663918     6882352\n",
       "74657  0.480274  0.446461  0.073265     6884758\n",
       "74658  0.011743  0.110521  0.877736     6924212\n",
       "\n",
       "[74659 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
